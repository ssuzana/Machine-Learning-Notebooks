{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTrSqiowiOk3qLnY36xm5j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Intro \n","\n","Movie recommendation can be framed as a machine learning classification problem. If it is predicted that you like a movie, for example, then it will be on your recommended list, otherwise, it won't. Predicting whether a person likes a movie is also a binary classification problem."],"metadata":{"id":"EIfsNkf7TY39"}},{"cell_type":"markdown","source":["# Bayes' theorem \n","\n","Let $A$ and $B$ denote two events. In Bayes' theorem, $P(A |B)$ is the probability that $A$ occurs given that $B$ is true. It can be computed as follows:\n","$$P(A |B) = \\frac{P(B |A)\\ P(A)}{P(B)}$$\n","where:\n"," * $P(A |B)$ is called the *likelihood*\n"," * $P(B |A)$ is called the *posterior* (probability)\n"," * $P(A)$ is called the *prior* (probability)\n"," * $P(B)$ is called the *evidence*.\n"],"metadata":{"id":"nXhOvULxZ0Cc"}},{"cell_type":"markdown","source":["# Naïve Bayes classifier\n","\n","* What Naïve Bayes does:\n","\n","  * It maps the **probability of observed input features given a possible class** to the **probability of the class given observed pieces of evidence** based on Bayes' theorem.\n","\n","  * It simplifies probability computation by assuming that predictive features are mutually independent.\n","\n","* Given a feature vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$, the goal of Naïve Bayes is to determine the probabilities that $\\mathbf{x}$ belongs to each of $K$ possible classes $y_1,y_2,\\ldots, y_K$. That is, \n","$$P(y_k|\\ \\mathbf{x}), \\text{ where } k=1,2,\n","\\ldots K$$\n","\n","* By Bayes's theorem, \n","$$P(y_k | \\mathbf{x})=\\frac{P(\\mathbf{x}|\\ y_k)P(y_k)}{P(\\mathbf{x})}$$\n","where:\n","  * $P(\\mathbf{x}|\\ y_k)=P(x_1, x_2, \\ldots, x_n|\\ y_k)$ is the joint distribution of the $n$ features $ x_1, x_2, \\ldots, x_n$, given that the sample belongs to class $y_k$. This is how likely the features with such values co-occur.\n","\n","  * $P(y_k|\\ \\mathbf{x})$, in contrast to $P(y_k)$, has extra knowledge of data sample $\\mathbf{x}$.\n","\n","  * $P(y_k)$ portrays how classes are distributed. It can be either predetermined (usually in a uniform manner where each class has an equal chance of occurence) or learned from a set of training examples.\n","\n","  * $P(\\mathbf{x})$ only depends on the overall distribution of features, which is not specific to certain classes and can be treated as a normalization constant, and thus\n","$$P(y_k | \\mathbf{x}) \\propto P(\\mathbf{x}|\\ y_K)P(y_k)$$.\n","where $\\propto$ denotes \"proportional\".\n","\n","\n","* Under the feature independence assumption, the joint conditional distribution of the $n$ features $x_1, x_2, \\ldots, x_n$ can be expressed as the product of individual feature conditional distributions:\n","$$P(x_1, x_2, \\ldots, x_n|\\ y_k) = P(x_1|\\ y_k)\\cdot P(x_2|\\ y_k)\\cdot \\ldots \\cdot P(x_n|\\ y_k)$$\n","\n","* Then:\n","$$P(y_k | \\mathbf{x}) \\propto P(x_1|\\ y_k)\\cdot P(x_2|\\ y_k)\\cdot \\ldots \\cdot P(x_n|\\ y_k)\\cdot P(y_k)$$.\n","\n","\n","\n"],"metadata":{"id":"qD0x3j51YdnQ"}},{"cell_type":"markdown","source":["# Simplified example of movie recommendation\n","\n","* Given four users, whether they like each of three movies, $m_1, m_2, m_3$ (indicated as `1` or `0`), and whether they like a target movie (denoted as event `Y`) or not (denoted as event `N`), as shown in the following table, we are asked to predict how likely it is that another user will like that movie:\n","\n","\n","\n","\n"," |    ID       |$m_1$| $m_2$| $m_3$| The user likes the target movie|\n"," | ----------- |----|----|----|:----:|\n"," |   1         | 0  |1|1|Y|\n"," |   2         | 0  |0|1|N|\n"," |   3         | 0  |0|0|Y|\n"," |   4         | 1  |1|0|Y|\n"," |   5         | 1  |1|0| ?|\n","\n","\n"," * Whether users like three movies, $m_1, m_2, m_3$, are features (signals) that we can utilize to predict the target class. \n","\n","* The training data we have are the four samples with both ratings and target information.\n","\n","* We want to calculate the probability that the user with ID=5 likes the target movie. That is, we want $P(Y|\\ \\mathbf{x}=(1,1,0))$.\n","\n","* The prior probabilities are:\n","$$ P(Y)= \\frac{3}{4} \\text{ and } P(N) = \\frac{1}{4}$$\n","\n","*  We will denote the event that a user likes the three movies or not as $f_1$, $f_2$, $f_3$, respectively. \n","\n","* Since $f_1=1$ was not seen in the $N$ class, we have $P(f_1 = 1|\\ N)=0$\n","Consequently, we will get $P(N|\\ \\mathbf{x})=0$, which means we will recklessly predict class = `Y`. To eliminate the zero-multiplication factor, the unknown likelihood, we usually assign an initial value of 1 to each feature, that is, we start counting each possible value of a feature from one. This technique is also known as **Laplace smoothing**.\n","\n","See https://courses.cs.washington.edu/courses/cse446/20wi/Section7/naive-bayes.pdf\n","\n","$$  P(f_1 = 1|\\ Y)= , P(f_2 = 1|\\ Y), \\text{ and } P(f_3 = 0|\\ Y)$$\n","$$  P(f_1 = 1|\\ N)=0, P(f_2 = 1|\\ N), \\text{ and } P(f_3 = 0|\\ N)$$\n"],"metadata":{"id":"l70kLzUjDWxA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DPpVxY_SGZG"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# References \n","\n","* [Python Machine Learning By Example - Third Edition,\n","by Yuxi (Hayden) Liu](https://www.packtpub.com/product/python-machine-learning-by-example-third-edition/9781800209718)"],"metadata":{"id":"b_CgwJLFuUFk"}},{"cell_type":"code","source":[],"metadata":{"id":"Dm3LwhqGulzD"},"execution_count":null,"outputs":[]}]}